# app_chat_dashboard_stream.py
# Chat / Logs / Charts ëŒ€ì‹œë³´ë“œ + Responses API ìŠ¤íŠ¸ë¦¬ë°(.stream) + íŒŒì¼ ì—…ë¡œë“œ(í…ìŠ¤íŠ¸ ì»¨í…ìŠ¤íŠ¸) í†µí•© ì˜ˆì œ

import os, time, textwrap, datetime as dt
import pandas as pd
import plotly.graph_objects as go
import streamlit as st
from dotenv import load_dotenv, find_dotenv
from openai import OpenAI

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 0) í™˜ê²½ì„¤ì •: dotenv â†’ OPENAI_API_KEY
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv(find_dotenv())
OPENAI_API_KEY = (os.getenv("OPENAI_API_KEY") or "").strip()

st.set_page_config(page_title="Chat + Logs + Charts (Streaming)", page_icon="ğŸ’¬", layout="wide")
st.title("ğŸ’¬ Chat Dashboard (Streamlit x OpenAI, Streaming)")

if not OPENAI_API_KEY or not OPENAI_API_KEY.startswith("sk-"):
    st.error("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
    st.stop()

client = OpenAI(api_key=OPENAI_API_KEY)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1) Session State ì´ˆê¸°í™”
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if "messages" not in st.session_state:
    st.session_state.messages = []   # ì±„íŒ… ë§í’ì„  í‘œì‹œìš©(ê°„ë‹¨)
if "logs" not in st.session_state:
    st.session_state.logs = []       # [{ts, model, domain, temp, chars_in, chars_out, t_ms, tok_in, tok_out, answer, question}]
if "sys_prompt" not in st.session_state:
    st.session_state.sys_prompt = textwrap.dedent("""
    You are a helpful assistant.
    - ë‹µì„ ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  ë§í•©ë‹ˆë‹¤.
    - ë¶ˆí™•ì‹¤í•œ ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    - ê°„ê²°í•˜ê³  ë‹¨ê³„ì ìœ¼ë¡œ ì„¤ëª…í•©ë‹ˆë‹¤.
    """)
if "upload_text" not in st.session_state:
    st.session_state.upload_text = ""   # ì—…ë¡œë“œëœ í…ìŠ¤íŠ¸(.txt) ë‚´ìš©ì„ ì €ì¥

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2) ì‚¬ì´ë“œë°” (ëª¨ë¸/ì˜¨ë„/ë„ë©”ì¸ + Streaming í† ê¸€)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with st.sidebar:
    st.subheader("âš™ï¸ Settings")
    model = st.selectbox(
        "Model",
        options=["gpt-4o-mini", "gpt-4o", "gpt-4.1", "gpt-3.5-turbo"],
        index=0,
    )
    temperature = st.slider("Temperature", 0.0, 1.0, 0.3, 0.1)
    domain = st.radio("Domain(ì—­í•  í”„ë¦¬ì…‹)", ["ì¼ë°˜", "ì—¬í–‰ì¶”ì²œ", "ì‹ë‹¨ì½”ì¹˜", "í•™ìŠµíŠœí„°"], index=0)
    streaming = st.toggle("ğŸ”´ Streaming ëª¨ë“œ", value=True, help="ì¼œë©´ ì‹¤ì‹œê°„ìœ¼ë¡œ í† í°ì´ í‘œì‹œë©ë‹ˆë‹¤.")
    use_uploaded = st.checkbox("ì—…ë¡œë“œí•œ í…ìŠ¤íŠ¸ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©", value=True, help="ì—…ë¡œë“œëœ .txt ë‚´ìš©ì„ í”„ë¡¬í”„íŠ¸ì— í¬í•¨")
    st.divider()
    st.caption("ğŸ”’ API Key: í™˜ê²½ë³€ìˆ˜(.env) ë¡œë“œë¨ (í™”ë©´ ë¹„ë…¸ì¶œ)")

DOMAIN_GUIDE = {
    "ì¼ë°˜": "",
    "ì—¬í–‰ì¶”ì²œ": "ì—¬í–‰í”Œë˜ë„ˆ ì—­í• ë¡œ, 2~3ê°œ í›„ë³´ ì¼ì •ê³¼ ì¥ë‹¨ì ì„ ì œì‹œí•©ë‹ˆë‹¤.",
    "ì‹ë‹¨ì½”ì¹˜": "ì˜ì–‘ ì½”ì¹˜ ì—­í• ë¡œ, ì•Œë ˆë¥´ê¸°/ì„ í˜¸ë¥¼ ì§ˆë¬¸í•˜ê³  1ì¼ ì‹ë‹¨ì„ ì œì‹œí•©ë‹ˆë‹¤.",
    "í•™ìŠµíŠœí„°": "í•™ìŠµ íŠœí„° ì—­í• ë¡œ, ì˜ˆì œâ†’ì„¤ëª…â†’í€´ì¦ˆâ†’ìš”ì•½ ìˆœì„œë¡œ ê°€ë¥´ì¹©ë‹ˆë‹¤.",
}
sys_prompt = st.session_state.sys_prompt + ("\n" + DOMAIN_GUIDE.get(domain, ""))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3) ìƒë‹¨ ì•ˆë‚´(01_text ì‘ìš©)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with st.expander("ğŸ“˜ ì‹œìŠ¤í…œ ê°€ì´ë“œ & ìƒ˜í”Œ í‘œì‹œ", expanded=False):
    st.markdown("""
- **ì—­í• (System Prompt)**ì€ ì¢Œì¸¡ Domain ì„ íƒì— ë”°ë¼ ë³´ê°•ë©ë‹ˆë‹¤.
- ì•„ë˜ëŠ” ì½”ë“œ/ìˆ˜ì‹ ì˜ˆì‹œì…ë‹ˆë‹¤.
""")
    st.code(
        "def few_shot_rule():\n    return 'ê°„ê²°í•˜ê²Œ, ë‹¨ê³„ë³„ë¡œ, ì˜ˆì‹œì™€ í•¨ê»˜ ì„¤ëª…'\n",
        language="python",
    )
    st.latex(r"x=\frac{-b\pm\sqrt{b^2-4ac}}{2a}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4) íƒ­ êµ¬ì„±: Chat / Logs / Charts
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
tab_chat, tab_logs, tab_charts = st.tabs(["ğŸ’¬ Chat", "ğŸ§¾ Logs", "ğŸ“ˆ Charts"])

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4-1) Chat íƒ­ (ì¢Œ: ëŒ€í™”, ìš°: KPI/ì›ì‹œì‘ë‹µ + íŒŒì¼ ì—…ë¡œë“œ)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with tab_chat:
    col_left, col_right = st.columns([2, 1])

    with col_left:
        st.subheader("ëŒ€í™”")
        # ì´ì „ ë©”ì‹œì§€ ì¶œë ¥
        for m in st.session_state.messages:
            with st.chat_message(m["role"]):
                st.markdown(m["content"])

        # ì…ë ¥
        if prompt := st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”"):
            # ì‚¬ìš©ì ë©”ì‹œì§€
            st.session_state.messages.append({"role": "user", "content": prompt})
            with st.chat_message("user"):
                st.markdown(prompt)

            start = time.perf_counter()
            try:
                # ì‹œìŠ¤í…œ+ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ í•©ì„± (ì—…ë¡œë“œ í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ì»¨í…ìŠ¤íŠ¸ë¡œ ì¶”ê°€)
                context_block = ""
                if use_uploaded and st.session_state.upload_text.strip():
                    # ë„ˆë¬´ ê¸¸ë©´ ì ˆë‹¨ (ëª¨ë¸ í† í° ë³´í˜¸)
                    ctx = st.session_state.upload_text.strip()
                    if len(ctx) > 4000:
                        ctx = ctx[:4000] + "\n...[truncated]"
                    context_block = f"\n[CONTEXT]\n{ctx}\n"

                composed = f"[SYSTEM]\n{sys_prompt}{context_block}\n[USER]\n{prompt}"

                if streaming:
                    # â”€â”€ ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    with st.chat_message("assistant"):
                        placeholder = st.empty()
                        chunks = []

                        with client.responses.stream(
                            model=model,
                            input=composed,
                            temperature=temperature,
                        ) as stream:
                            for event in stream:
                                if event.type == "response.output_text.delta":
                                    chunks.append(event.delta)
                                    placeholder.markdown("".join(chunks))
                            stream.until_done()

                        answer = "".join(chunks) or "(ì‘ë‹µ ì—†ìŒ)"
                else:
                    # â”€â”€ ë¹„-ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    resp = client.responses.create(
                        model=model,
                        input=composed,
                        temperature=temperature,
                    )
                    answer = getattr(resp, "output_text", None) or str(resp)
                    with st.chat_message("assistant"):
                        st.markdown(answer)

                dur_ms = int((time.perf_counter() - start) * 1000)

                # ë©”ì‹œì§€/ë¡œê·¸ ì ì¬
                st.session_state.messages.append({"role": "assistant", "content": answer})
                st.session_state.logs.append({
                    "ts": dt.datetime.now().isoformat(timespec="seconds"),
                    "model": model,
                    "domain": domain,
                    "temp": temperature,
                    "chars_in": len(prompt),
                    "chars_out": len(answer),
                    "t_ms": dur_ms,
                    "tok_in": None,
                    "tok_out": None,
                    "answer": answer,
                    "question": prompt,
                })

            except Exception as e:
                with st.chat_message("assistant"):
                    st.error(f"OpenAI í˜¸ì¶œ ì‹¤íŒ¨: {e}")

    with col_right:
        st.subheader("í†µê³„ / ì›ì‹œ ì‘ë‹µ / ì—…ë¡œë“œ")

        # â”€â”€ KPI (02_data: metric)
        df = pd.DataFrame(st.session_state.logs)
        turns = len(df)
        avg_ms = int(df["t_ms"].mean()) if turns else 0
        total_chars = int(df["chars_out"].sum()) if turns else 0

        c1, c2, c3 = st.columns(3)
        c1.metric("Turns", turns)
        c2.metric("Avg Latency(ms)", avg_ms)
        c3.metric("Out Chars", total_chars)

        # â”€â”€ ìµœê·¼ ì‘ë‹µ ì›ì‹œ(ê°„ë‹¨) (02_data: json)
        if turns:
            st.caption("ìµœê·¼ ì‘ë‹µ ìš”ì•½(JSON ëŠë‚Œ)")
            st.json({
                "answer": df.iloc[-1]["answer"],
                "elapsed_ms": df.iloc[-1]["t_ms"],
                "model": df.iloc[-1]["model"],
                "domain": df.iloc[-1]["domain"],
            })

        st.divider()
        # â”€â”€ íŒŒì¼ ì—…ë¡œë“œ (ìš”ì²­í•˜ì‹  ì½”ë“œ í¬í•¨) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        st.markdown("**ğŸ“ íŒŒì¼ ì—…ë¡œë“œ**")
        uploaded_file = st.file_uploader("íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["txt", "pdf", "png", "jpg"])
        if uploaded_file is not None:
            st.write("íŒŒì¼ ì´ë¦„:", uploaded_file.name)
            st.write("íŒŒì¼ í¬ê¸°:", uploaded_file.size, "bytes")

            # í…ìŠ¤íŠ¸ íŒŒì¼ â†’ ë¯¸ë¦¬ë³´ê¸° + ì»¨í…ìŠ¤íŠ¸ ì €ì¥
            if uploaded_file.type == "text/plain":
                # íŒŒì¼ í¬ì¸í„° ì£¼ì˜: ì—¬ëŸ¬ ë²ˆ ì½ì„ ìˆ˜ ìˆë„ë¡ í•„ìš” ì‹œ seek(0)
                content = uploaded_file.read().decode("utf-8", errors="ignore")
                st.text_area("í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°", content, height=200)
                st.session_state.upload_text = content  # ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©
                st.success("ì´ í…ìŠ¤íŠ¸ëŠ” ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤. (ì‚¬ì´ë“œë°” 'ì—…ë¡œë“œ í…ìŠ¤íŠ¸ ì‚¬ìš©' ì²´í¬ ìƒíƒœ)")
            # ì´ë¯¸ì§€ íŒŒì¼ â†’ ë¯¸ë¦¬ë³´ê¸°
            elif uploaded_file.type in ("image/png", "image/jpeg"):
                st.image(uploaded_file, caption=uploaded_file.name, use_column_width=True)
                st.info("ì´ë¯¸ì§€ëŠ” í˜„ì¬ ì»¨í…ìŠ¤íŠ¸ì— ìë™ ì£¼ì…í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            # PDF â†’ ë©”íƒ€ ì •ë³´ë§Œ
            elif uploaded_file.type == "application/pdf":
                st.info("PDF í…ìŠ¤íŠ¸ ì¶”ì¶œì€ ì´ ì˜ˆì œì— í¬í•¨ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. (RAG ì‹¤ìŠµì—ì„œ í™•ì¥ ê°€ëŠ¥)")
            else:
                st.warning("ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤.")

        st.divider()
        st.info("ë„ì›€ë§: Domain/Temperature/Streaming, ì—…ë¡œë“œ í…ìŠ¤íŠ¸ ì‚¬ìš©ì„ ë°”ê¿”ê°€ë©° ì‘ë‹µ ë³€í™”ë¥¼ ê´€ì°°í•˜ì„¸ìš”.")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4-2) Logs íƒ­: ëˆ„ì  ë¡œê·¸ í…Œì´ë¸” + CSV ë‹¤ìš´ë¡œë“œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with tab_logs:
    st.subheader("ëŒ€í™” ë¡œê·¸")
    df = pd.DataFrame(st.session_state.logs)
    if df.empty:
        st.warning("ì•„ì§ ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤. Chat íƒ­ì—ì„œ ëŒ€í™”ë¥¼ ì‹œì‘í•˜ì„¸ìš”.")
    else:
        st.dataframe(df, use_container_width=True)
        csv = df.to_csv(index=False).encode("utf-8")
        st.download_button("CSVë¡œ ë‹¤ìš´ë¡œë“œ", data=csv, file_name="chat_logs.csv", mime="text/csv")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4-3) Charts íƒ­: Plotlyë¡œ ì§€í‘œ ì‹œê°í™”
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with tab_charts:
    st.subheader("ì„¸ì…˜ ì§€í‘œ ì‹œê°í™” (Plotly)")
    df = pd.DataFrame(st.session_state.logs)
    if df.empty:
        st.info("ì‹œê°í™”í•  ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        x = list(range(1, len(df) + 1))

        fig1 = go.Figure(data=go.Scatter(x=x, y=df["t_ms"], mode="lines+markers"))
        fig1.update_layout(title="ì‘ë‹µ ì‹œê°„(ms) ì¶”ì´", xaxis_title="Turn", yaxis_title="ms")
        st.plotly_chart(fig1, use_container_width=True)

        fig2 = go.Figure(data=go.Scatter(x=x, y=df["chars_out"], mode="markers"))
        fig2.update_layout(title="ì¶œë ¥ ê¸¸ì´(ë¬¸ì ìˆ˜)", xaxis_title="Turn", yaxis_title="chars_out")
        st.plotly_chart(fig2, use_container_width=True)

        if "tok_out" in df.columns and df["tok_out"].notna().any():
            fig3 = go.Figure(data=go.Bar(x=x, y=df["tok_out"]))
            fig3.update_layout(title="ì¶œë ¥ í† í° ìˆ˜(ìˆì„ ë•Œ)", xaxis_title="Turn", yaxis_title="tok_out")
            st.plotly_chart(fig3, use_container_width=True)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5) ë³´ì•ˆ/ë°°í¬ ë©”ëª¨
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with st.expander("ğŸ”’ ë³´ì•ˆ/ë°°í¬ ì²´í¬ë¦¬ìŠ¤íŠ¸", expanded=False):
    st.markdown("""
- API KeyëŠ” ì½”ë“œì— ì§ì ‘ ë„£ì§€ ë§ê³  **.env** ë˜ëŠ” ë°°í¬ í™˜ê²½ ë³€ìˆ˜ë¡œ ê´€ë¦¬
- Git ì»¤ë°‹ ê¸ˆì§€(.gitignore)
- Streamlit Cloud ì‚¬ìš© ì‹œ **Secrets** UIì— ë“±ë¡ ê°€ëŠ¥
- ì¸ì¦ì´ í•„ìš”í•˜ë©´ OIDC/í”„ë¡ì‹œ ì•ë‹¨ ê³ ë ¤
""")
