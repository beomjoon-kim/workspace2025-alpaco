# app_chat_dashboard_rag.py
# Chat / Logs / Charts ëŒ€ì‹œë³´ë“œ + ì—…ë¡œë“œ ì»¨í…ìŠ¤íŠ¸
# + PDF ì œëª©/êµ¬ì—­ Chunking â†’ Embedding Index â†’ Query-time Retrieval(RAG)
# + ì´ë¯¸ì§€ OCR, TXT/PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ê·¸ëŒ€ë¡œë„ ì‚¬ìš© ê°€ëŠ¥
import os, io, time, textwrap, hashlib, json, datetime as dt, re, pathlib
from typing import List, Dict, Any
import numpy as np
import pandas as pd
import plotly.graph_objects as go
import streamlit as st
from dotenv import load_dotenv, find_dotenv
from openai import OpenAI

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì¶”ê°€: PDF / ì´ë¯¸ì§€ OCR ìœ í‹¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
try:
    import pdfplumber  # PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
except Exception:
    pdfplumber = None

try:
    from PIL import Image
    import pytesseract  # ì´ë¯¸ì§€ OCR (ì‹œìŠ¤í…œì— Tesseract ì„¤ì¹˜ í•„ìš”)
except Exception:
    Image = None
    pytesseract = None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 0) í™˜ê²½ì„¤ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv(find_dotenv())
OPENAI_API_KEY = (os.getenv("OPENAI_API_KEY") or "").strip()
EMBED_MODEL = "text-embedding-3-small"   # ë¹„ìš©â†“, 1536-d
GEN_MODELS = ["gpt-4o-mini", "gpt-4o", "gpt-4.1", "gpt-3.5-turbo"]

st.set_page_config(page_title="RAG Chat Dashboard", page_icon="ğŸ“š", layout="wide")
st.title("ğŸ“š PDF ì„¹ì…˜ RAG + Chat Dashboard (Streamlit x OpenAI)")

if not OPENAI_API_KEY or not OPENAI_API_KEY.startswith("sk-"):
    st.error("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
    st.stop()

client = OpenAI(api_key=OPENAI_API_KEY)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1) Session State
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if "messages" not in st.session_state:
    st.session_state.messages = []
if "logs" not in st.session_state:
    st.session_state.logs = []
if "sys_prompt" not in st.session_state:
    st.session_state.sys_prompt = textwrap.dedent("""
    You are a helpful assistant.
    - ë‹µì„ ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  ë§í•©ë‹ˆë‹¤.
    - ë¶ˆí™•ì‹¤í•œ ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    - ê°„ê²°í•˜ê³  ë‹¨ê³„ì ìœ¼ë¡œ ì„¤ëª…í•©ë‹ˆë‹¤.
    """)
if "upload_text" not in st.session_state:
    st.session_state.upload_text = ""      # ì¼ë°˜ ì»¨í…ìŠ¤íŠ¸(í…ìŠ¤íŠ¸/ocr/pdf full)
if "rag_sections" not in st.session_state:
    st.session_state.rag_sections = []     # [{title, content, page, section_id}]
if "rag_index" not in st.session_state:
    st.session_state.rag_index = None      # {"emb": np.ndarray, "meta": [...]}
if "rag_file_sig" not in st.session_state:
    st.session_state.rag_file_sig = None   # ì—…ë¡œë“œ PDF íŒŒì¼ í•´ì‹œ(ìºì‹œ ë¬´ê²°ì„±ìš©)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2) ì‚¬ì´ë“œë°”
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with st.sidebar:
    st.subheader("âš™ï¸ Settings")
    model = st.selectbox("Model", options=GEN_MODELS, index=0)
    temperature = st.slider("Temperature", 0.0, 1.0, 0.3, 0.1)
    domain = st.radio("Domain(ì—­í•  í”„ë¦¬ì…‹)", ["ì¼ë°˜", "ì—¬í–‰ì¶”ì²œ", "ì‹ë‹¨ì½”ì¹˜", "í•™ìŠµíŠœí„°"], index=0)
    streaming = st.toggle("ğŸ”´ Streaming ëª¨ë“œ", value=True)
    st.divider()

    st.markdown("**ì»¨í…ìŠ¤íŠ¸ ì˜µì…˜**")
    use_uploaded = st.checkbox("ì—…ë¡œë“œ í…ìŠ¤íŠ¸ ë‹¨ìˆœ ì£¼ì…(ì›ë³¸)", value=True)
    ctx_limit = st.number_input("ë‹¨ìˆœ ì»¨í…ìŠ¤íŠ¸ ìµœëŒ€ ê¸¸ì´(ë¬¸ì)", min_value=500, max_value=20000, value=4000, step=500)

    st.markdown("**RAG ì˜µì…˜(PDF)**")
    use_rag = st.checkbox("PDF ì„¹ì…˜ RAG ì‚¬ìš©", value=True)
    top_k = st.number_input("RAG Top-K", min_value=1, max_value=10, value=4, step=1)
    rag_ctx_limit = st.number_input("RAG ì»¨í…ìŠ¤íŠ¸ ìµœëŒ€ ê¸¸ì´(ë¬¸ì)", min_value=500, max_value=20000, value=5000, step=500)

    st.caption("RAGì™€ ë‹¨ìˆœ ì£¼ì…ì€ í•¨ê»˜ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. (í”„ë¡¬í”„íŠ¸ì— [RAG] ë¸”ë¡, [CONTEXT] ë¸”ë¡ ìˆœìœ¼ë¡œ ì¶”ê°€)")

DOMAIN_GUIDE = {
    "ì¼ë°˜": "",
    "ì—¬í–‰ì¶”ì²œ": "ì—¬í–‰í”Œë˜ë„ˆ ì—­í• ë¡œ, 2~3ê°œ í›„ë³´ ì¼ì •ê³¼ ì¥ë‹¨ì ì„ ì œì‹œí•©ë‹ˆë‹¤.",
    "ì‹ë‹¨ì½”ì¹˜": "ì˜ì–‘ ì½”ì¹˜ ì—­í• ë¡œ, ì•Œë ˆë¥´ê¸°/ì„ í˜¸ë¥¼ ì§ˆë¬¸í•˜ê³  1ì¼ ì‹ë‹¨ì„ ì œì‹œí•©ë‹ˆë‹¤.",
    "í•™ìŠµíŠœí„°": "í•™ìŠµ íŠœí„° ì—­í• ë¡œ, ì˜ˆì œâ†’ì„¤ëª…â†’í€´ì¦ˆâ†’ìš”ì•½ ìˆœì„œë¡œ ê°€ë¥´ì¹©ë‹ˆë‹¤.",
}
sys_prompt = st.session_state.sys_prompt + ("\n" + DOMAIN_GUIDE.get(domain, ""))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3) ì—…ë¡œë“œ íŒŒì¼ í…ìŠ¤íŠ¸ ì¶”ì¶œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def extract_text_from_upload(uploaded_file) -> str:
    """
    ì—…ë¡œë“œëœ íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    - text/plain: UTF-8 ë””ì½”ë“œ
    - application/pdf: pdfplumberë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    - image/*: pytesseractë¡œ OCR (ê¸°ë³¸ en, í•œêµ­ì–´ kor ì˜µì…˜)
    """
    if uploaded_file is None:
        return ""

    mime = uploaded_file.type or ""
    name = getattr(uploaded_file, "name", "uploaded")

    try:
        if mime == "text/plain":
            uploaded_file.seek(0)
            return uploaded_file.read().decode("utf-8", errors="ignore")

        if mime == "application/pdf":
            if not pdfplumber:
                st.warning("pdfplumberê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•Šì•„ PDF í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                return ""
            uploaded_file.seek(0)
            text_chunks = []
            with pdfplumber.open(uploaded_file) as pdf:
                for page in pdf.pages:
                    text = page.extract_text() or ""
                    if text.strip():
                        text_chunks.append(text)
            return "\n\n".join(text_chunks).strip()

        if mime in ("image/png", "image/jpeg"):
            if not (Image and pytesseract):
                st.warning("Pillow/pytesseractê°€ ì—†ì–´ì„œ ì´ë¯¸ì§€ OCRì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return ""
            uploaded_file.seek(0)
            img = Image.open(uploaded_file)

            # OCR ì–¸ì–´ ì„ íƒ
            # í•œêµ­ì–´ ë°ì´í„°(kor.traineddata)ê°€ ì„¤ì¹˜ë˜ì–´ ìˆìœ¼ë©´ í•œêµ­ì–´ OCR ì‹œë„
            try:
                text = pytesseract.image_to_string(img, lang="kor+eng")
            except Exception:
                text = pytesseract.image_to_string(img)  # fallback: ì˜ì–´ë§Œ
            return text

    except Exception as e:
        st.warning(f"íŒŒì¼ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")

    st.info(f"ë¯¸ì§€ì› íŒŒì¼ í˜•ì‹ ë˜ëŠ” í…ìŠ¤íŠ¸ë¥¼ ì°¾ì§€ ëª»í•¨: {name} ({mime})")
    return ""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4) PDF â†’ ì„¹ì…˜ Chunking (ì œëª©/êµ¬ì—­ ë‹¨ìœ„)
#    - ë‹¨ìˆœ í…ìŠ¤íŠ¸ ê¸°ë°˜ ê·œì¹™: ë²ˆí˜¸í˜• ì œëª©, ëŒ€ë¬¸ì/Title Case, ë¹ˆì¤„ ê¸°ì¤€ ë“±
#    - ì‹¤íŒ¨ ì‹œ ë¬¸ì¥ ë‹¨ìœ„ ìŠ¬ë¼ì´ë”© ìœˆë„ìš°ë¡œ ë³´ìˆ˜ì ìœ¼ë¡œ ë¶„í• 
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
HEADING_PATTERNS = [
    r"^\s*\d+(\.\d+)*\s+.+",          # 1 / 1.1 / 2.3.4 í˜•íƒœ
    r"^\s*[IVXLCM]+\.\s+.+",          # ë¡œë§ˆìˆ«ì. Title
    r"^\s*[A-Z][A-Z\s\-:]{4,}$",      # ì „ë¶€ ëŒ€ë¬¸ì ê³„ì—´ ì œëª©
]

def naive_split_sections(full_text: str) -> List[Dict[str, Any]]:
    lines = full_text.splitlines()
    sections = []
    cur_title = "Introduction"
    cur_buf = []
    cur_start_page = None  # í…ìŠ¤íŠ¸ ì¶”ì¶œ ë°©ì‹ì—ì„  ì •í™• í˜ì´ì§€ ë§¤í•‘ ì–´ë ¤ì›€ â†’ None ì²˜ë¦¬

    def flush():
        if cur_buf:
            sections.append({
                "title": cur_title.strip(),
                "content": "\n".join(cur_buf).strip(),
                "page": cur_start_page,
            })

    heading_regex = re.compile("|".join(HEADING_PATTERNS), flags=re.IGNORECASE)

    for ln in lines:
        if heading_regex.match(ln.strip()):
            # ìƒˆ ì„¹ì…˜ ì‹œì‘
            if cur_buf:
                flush()
                cur_buf = []
            cur_title = ln.strip()
        else:
            cur_buf.append(ln)
    # ë§ˆì§€ë§‰ ì„¹ì…˜ flush
    flush()

    # ì„¹ì…˜ì´ ë„ˆë¬´ ì ê±°ë‚˜ ì œëª© ê°ì§€ê°€ ì‹¤íŒ¨í•˜ë©´, ë¬¸ì¥ ìŠ¬ë¼ì´ë”© ìœˆë„ìš°ë¡œ ë¶„í• 
    if len(sections) <= 1:
        sections = []
        sents = re.split(r"(?<=[\.\?\!])\s+", full_text)
        win = 8              # ë¬¸ì¥ 8ê°œ ì •ë„ì”©
        stride = 6           # ê²¹ì¹˜ê²Œ
        for i in range(0, len(sents), stride):
            chunk = " ".join(sents[i:i+win]).strip()
            if len(chunk) > 100:
                sections.append({
                    "title": f"Chunk {i//stride+1}",
                    "content": chunk,
                    "page": None,
                })
    # section_id ë¶€ì—¬
    for idx, s in enumerate(sections, start=1):
        s["section_id"] = idx
    return sections

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5) Embedding Index (ë¡œì»¬ ìºì‹œ)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CACHE_DIR = pathlib.Path(".rag_cache")
CACHE_DIR.mkdir(exist_ok=True)

def file_signature(file_bytes: bytes) -> str:
    return hashlib.sha256(file_bytes).hexdigest()

def build_rag_index(sections: List[Dict[str, Any]], cache_key: str):
    """sections â†’ ì„ë² ë”© ë°°ì—´(np.float32) + ë©”íƒ€ë°ì´í„° ë¦¬ìŠ¤íŠ¸"""
    texts = [f"{s['title']}\n{s['content']}" for s in sections]
    # ì„ë² ë”© í˜¸ì¶œ
    resp = client.embeddings.create(model=EMBED_MODEL, input=texts)
    emb = np.array([e.embedding for e in resp.data], dtype=np.float32)
    meta = [{"title": s["title"], "page": s["page"], "section_id": s["section_id"], "n_chars": len(s["content"])} for s in sections]

    # ìºì‹œ ì €ì¥
    np.save(CACHE_DIR / f"{cache_key}.npy", emb)
    with open(CACHE_DIR / f"{cache_key}.json", "w", encoding="utf-8") as f:
        json.dump({"meta": meta}, f, ensure_ascii=False)
    return {"emb": emb, "meta": meta}

def load_rag_index(cache_key: str):
    npy = CACHE_DIR / f"{cache_key}.npy"
    jsn = CACHE_DIR / f"{cache_key}.json"
    if npy.exists() and jsn.exists():
        emb = np.load(npy)
        meta = json.loads(jsn.read_text(encoding="utf-8"))["meta"]
        return {"emb": emb, "meta": meta}
    return None

def ensure_rag_index(pdf_bytes: bytes, sections: List[Dict[str, Any]]):
    sig = file_signature(pdf_bytes)
    cached = load_rag_index(sig)
    if cached:
        return sig, cached
    built = build_rag_index(sections, sig)
    return sig, built

def embed_query(q: str) -> np.ndarray:
    v = client.embeddings.create(model=EMBED_MODEL, input=[q]).data[0].embedding
    return np.array(v, dtype=np.float32)

def cosine_sim(a: np.ndarray, b: np.ndarray) -> np.ndarray:
    a_n = a / (np.linalg.norm(a, axis=-1, keepdims=True) + 1e-8)
    b_n = b / (np.linalg.norm(b, axis=-1, keepdims=True) + 1e-8)
    return np.dot(a_n, b_n.T)

def retrieve_sections(query: str, index: Dict[str, Any], sections: List[Dict[str, Any]], k: int = 4):
    if not index or "emb" not in index:
        return []
    qv = embed_query(query)
    sims = cosine_sim(qv.reshape(1, -1), index["emb"]).ravel()
    top_idx = np.argsort(-sims)[:k]
    items = []
    for i in top_idx:
        meta = index["meta"][i]
        sec = next((s for s in sections if s["section_id"] == meta["section_id"]), None)
        if not sec:
            continue
        items.append({
            "score": float(sims[i]),
            "title": meta["title"],
            "page": meta["page"],
            "content": sec["content"],
            "section_id": meta["section_id"],
        })
    return items

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 6) ìƒë‹¨ ê°€ì´ë“œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with st.expander("ğŸ“˜ ì‹œìŠ¤í…œ ê°€ì´ë“œ & ìƒ˜í”Œ í‘œì‹œ", expanded=False):
    st.markdown("""
- **PDF ì—…ë¡œë“œ í›„ [RAG ë¹Œë“œ]** ë²„íŠ¼ì„ ëˆ„ë¥´ë©´ *ì œëª©/êµ¬ì—­* ë‹¨ìœ„ë¡œ ë¶„í• â†’ì„ë² ë”© ìƒ‰ì¸í•©ë‹ˆë‹¤.
- ì§ˆë¬¸ ì‹œ Top-K ìœ ì‚¬ ì„¹ì…˜ì„ ì°¾ì•„ **[RAG]** ë¸”ë¡ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ì— ìë™ ì‚½ì…í•©ë‹ˆë‹¤.
- ë™ì‹œì— TXT/PDF/ì´ë¯¸ì§€ì—ì„œ ì¶”ì¶œí•œ **ì›ë³¸ í…ìŠ¤íŠ¸ ì „ì²´**ë¥¼ **[CONTEXT]** ë¸”ë¡ìœ¼ë¡œ ì¼ë¶€(ë¬¸ì í•œë„) ë§ë¶™ì¼ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.
""")
    st.code("def few_shot_rule():\n    return 'ê°„ê²°í•˜ê²Œ, ë‹¨ê³„ë³„ë¡œ, ì˜ˆì‹œì™€ í•¨ê»˜ ì„¤ëª…'\n", language="python")
    st.latex(r"x=\frac{-b\pm\sqrt{b^2-4ac}}{2a}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 7) íƒ­: Chat / Logs / Charts
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
tab_chat, tab_logs, tab_charts = st.tabs(["ğŸ’¬ Chat", "ğŸ§¾ Logs", "ğŸ“ˆ Charts"])

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 7-1) Chat íƒ­
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with tab_chat:
    col_left, col_right = st.columns([2, 1])

    with col_right:
        st.subheader("ì—…ë¡œë“œ & RAG")
        uploaded_file = st.file_uploader("ğŸ“ TXT/PDF/PNG/JPG ì—…ë¡œë“œ", type=["txt", "pdf", "png", "jpg", "jpeg"])
        pdf_bytes = None

        if uploaded_file is not None:
            st.write("íŒŒì¼:", uploaded_file.name, f"({uploaded_file.type}, {uploaded_file.size} bytes)")
            # 1) ì›ë³¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ(ë‹¨ìˆœ ì»¨í…ìŠ¤íŠ¸ ì£¼ì…ìš©)
            extracted = extract_text_from_upload(uploaded_file).strip()
            if extracted:
                st.text_area("ì¶”ì¶œ í…ìŠ¤íŠ¸(ë¯¸ë¦¬ë³´ê¸°)", extracted[:2000] + ("\n...[more]" if len(extracted) > 2000 else ""), height=180)
                st.session_state.upload_text = extracted
                st.success("ë‹¨ìˆœ ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„ ì™„ë£Œ")

            # 2) RAG: PDFë§Œ ì„¹ì…˜ ë¶„í•  ëŒ€ìƒ
            if (uploaded_file.type or "") == "application/pdf":
                uploaded_file.seek(0)
                pdf_bytes = uploaded_file.read()
                # ì„¹ì…˜ ìƒíƒœ ì´ˆê¸°í™”
                st.session_state.rag_sections = []
                st.session_state.rag_index = None
                st.session_state.rag_file_sig = None

                if st.button("ğŸ§© RAG ë¹Œë“œ (PDF ì„¹ì…˜ ë¶„í•  â†’ ì„ë² ë”© ìƒ‰ì¸)"):
                    # í…ìŠ¤íŠ¸ ê¸°ë°˜ ì„¹ì…˜ ë¶„í• 
                    if not extracted:
                        st.error("PDF í…ìŠ¤íŠ¸ ì¶”ì¶œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                    else:
                        sections = naive_split_sections(extracted)
                        if not sections:
                            st.error("ì„¹ì…˜ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                        else:
                            sig, index = ensure_rag_index(pdf_bytes, sections)
                            st.session_state.rag_sections = sections
                            st.session_state.rag_index = index
                            st.session_state.rag_file_sig = sig
                            st.success(f"RAG ë¹Œë“œ ì™„ë£Œ: {len(sections)}ê°œ ì„¹ì…˜, ì„ë² ë”© {index['emb'].shape}")

            # ê°€ì´ë“œ
            if pdfplumber is None:
                st.info("PDF í…ìŠ¤íŠ¸ ì¶”ì¶œì„ ì‚¬ìš©í•˜ë ¤ë©´ `pip install pdfplumber` í›„ ì•±ì„ ì¬ì‹œì‘í•˜ì„¸ìš”.")
            if pytesseract is None or Image is None:
                st.info("ì´ë¯¸ì§€ OCRì„ ì‚¬ìš©í•˜ë ¤ë©´ `pip install pillow pytesseract` + Tesseract ì„¤ì¹˜ í•„ìš”")

        st.divider()

        # ìµœê·¼ ì‘ë‹µ KPI/ì›ì‹œ ìš”ì•½
        df = pd.DataFrame(st.session_state.logs)
        turns = len(df)
        avg_ms = int(df["t_ms"].mean()) if turns else 0
        total_chars = int(df["chars_out"].sum()) if turns else 0

        c1, c2, c3 = st.columns(3)
        c1.metric("Turns", turns)
        c2.metric("Avg Latency(ms)", avg_ms)
        c3.metric("Out Chars", total_chars)

        if turns:
            st.caption("ìµœê·¼ ì‘ë‹µ ìš”ì•½")
            st.json({
                "answer": df.iloc[-1]["answer"],
                "elapsed_ms": df.iloc[-1]["t_ms"],
                "model": df.iloc[-1]["model"],
                "domain": df.iloc[-1]["domain"],
            })

    with col_left:
        st.subheader("ëŒ€í™”")

        # ê³¼ê±° ë©”ì‹œì§€
        for m in st.session_state.messages:
            with st.chat_message(m["role"]):
                st.markdown(m["content"])

        # ì…ë ¥ ë° ì‘ë‹µ
        if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (PDF RAG + ë‹¨ìˆœ ì»¨í…ìŠ¤íŠ¸ ì§€ì›)"):
            # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ/ì €ì¥
            st.session_state.messages.append({"role": "user", "content": prompt})
            with st.chat_message("user"):
                st.markdown(prompt)

            # â”€â”€ í”„ë¡¬í”„íŠ¸ í•©ì„±: [SYSTEM] + [RAG] + [CONTEXT] + [USER] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            rag_block = ""
            retrieved_items = []
            if use_rag and st.session_state.rag_index is not None and st.session_state.rag_sections:
                retrieved_items = retrieve_sections(prompt, st.session_state.rag_index, st.session_state.rag_sections, k=top_k)
                if retrieved_items:
                    # ê¸¸ì´ ì œí•œ ë‚´ì—ì„œ ì •ë¦¬
                    pieces = []
                    acc_len = 0
                    limit = int(rag_ctx_limit)
                    for it in retrieved_items:
                        head = f"### {it['title']} (sec:{it['section_id']}{', p.'+str(it['page']) if it['page'] else ''})\n"
                        body = it["content"].strip()
                        chunk_text = head + body + "\n"
                        if acc_len + len(chunk_text) > limit:
                            chunk_text = (head + body)[: max(0, limit-acc_len)] + "\n...[truncated]"
                            pieces.append(chunk_text)
                            break
                        pieces.append(chunk_text)
                        acc_len += len(chunk_text)
                    rag_block = "\n[RAG]\n" + "\n---\n".join(pieces) + "\n"

            context_block = ""
            if use_uploaded and st.session_state.upload_text.strip():
                ctx = st.session_state.upload_text.strip()
                if len(ctx) > ctx_limit:
                    ctx = ctx[:ctx_limit] + "\n...[truncated]"
                context_block = f"\n[CONTEXT]\n{ctx}\n"

            composed = f"[SYSTEM]\n{sys_prompt}{rag_block}{context_block}\n[USER]\n{prompt}"

            # â”€â”€ í˜¸ì¶œ
            start = time.perf_counter()
            try:
                if streaming:
                    with st.chat_message("assistant"):
                        placeholder = st.empty()
                        chunks = []
                        with client.responses.stream(
                            model=model,
                            input=composed,
                            temperature=temperature,
                        ) as stream:
                            for event in stream:
                                if event.type == "response.output_text.delta":
                                    chunks.append(event.delta)
                                    placeholder.markdown("".join(chunks))
                            stream.until_done()
                        answer = "".join(chunks) or "(ì‘ë‹µ ì—†ìŒ)"
                else:
                    resp = client.responses.create(model=model, input=composed, temperature=temperature)
                    answer = getattr(resp, "output_text", None) or str(resp)
                    with st.chat_message("assistant"):
                        st.markdown(answer)

                dur_ms = int((time.perf_counter() - start) * 1000)
                st.session_state.messages.append({"role": "assistant", "content": answer})
                st.session_state.logs.append({
                    "ts": dt.datetime.now().isoformat(timespec="seconds"),
                    "model": model,
                    "domain": domain,
                    "temp": temperature,
                    "chars_in": len(prompt),
                    "chars_out": len(answer),
                    "t_ms": dur_ms,
                    "tok_in": None,
                    "tok_out": None,
                    "answer": answer,
                    "question": prompt,
                })

                # RAG ë§¤ì¹­ ê²°ê³¼ í‘œë¡œ í‘œì‹œ
                if retrieved_items:
                    st.markdown("**ğŸ” RAG ë§¤ì¹­ ì„¹ì…˜ (ìœ ì‚¬ë„ ë†’ì€ ìˆœ)**")
                    show = []
                    for it in retrieved_items:
                        show.append({
                            "score": round(it["score"], 4),
                            "section_id": it["section_id"],
                            "title": it["title"],
                            "page": it["page"],
                            "chars": len(it["content"]),
                        })
                    st.dataframe(pd.DataFrame(show), use_container_width=True)

            except Exception as e:
                with st.chat_message("assistant"):
                    st.error(f"OpenAI í˜¸ì¶œ ì‹¤íŒ¨: {e}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 7-2) Logs
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with tab_logs:
    st.subheader("ëŒ€í™” ë¡œê·¸")
    df = pd.DataFrame(st.session_state.logs)
    if df.empty:
        st.warning("ì•„ì§ ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤. Chat íƒ­ì—ì„œ ëŒ€í™”ë¥¼ ì‹œì‘í•˜ì„¸ìš”.")
    else:
        st.dataframe(df, use_container_width=True)
        csv = df.to_csv(index=False).encode("utf-8")
        st.download_button("CSVë¡œ ë‹¤ìš´ë¡œë“œ", data=csv, file_name="chat_logs.csv", mime="text/csv")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 7-3) Charts
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with tab_charts:
    st.subheader("ì„¸ì…˜ ì§€í‘œ ì‹œê°í™” (Plotly)")
    df = pd.DataFrame(st.session_state.logs)
    if df.empty:
        st.info("ì‹œê°í™”í•  ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        x = list(range(1, len(df) + 1))
        fig1 = go.Figure(data=go.Scatter(x=x, y=df["t_ms"], mode="lines+markers"))
        fig1.update_layout(title="ì‘ë‹µ ì‹œê°„(ms) ì¶”ì´", xaxis_title="Turn", yaxis_title="ms")
        st.plotly_chart(fig1, use_container_width=True)

        fig2 = go.Figure(data=go.Scatter(x=x, y=df["chars_out"], mode="markers"))
        fig2.update_layout(title="ì¶œë ¥ ê¸¸ì´(ë¬¸ì ìˆ˜)", xaxis_title="Turn", yaxis_title="chars_out")
        st.plotly_chart(fig2, use_container_width=True)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 8) ë³´ì•ˆ/ë°°í¬ ë©”ëª¨
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with st.expander("ğŸ”’ ë³´ì•ˆ/ë°°í¬ ì²´í¬ë¦¬ìŠ¤íŠ¸", expanded=False):
    st.markdown("""
- API KeyëŠ” ì½”ë“œì— ì§ì ‘ ë„£ì§€ ë§ê³  **.env** ë˜ëŠ” ë°°í¬ í™˜ê²½ ë³€ìˆ˜ë¡œ ê´€ë¦¬
- Git ì»¤ë°‹ ê¸ˆì§€(.gitignore)
- Streamlit Cloud ì‚¬ìš© ì‹œ **Secrets** UIì— ë“±ë¡ ê°€ëŠ¥
- ì¸ì¦ì´ í•„ìš”í•˜ë©´ OIDC/í”„ë¡ì‹œ ì•ë‹¨ ê³ ë ¤
""")
